{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial comparison with DLS\n",
    "===============\n",
    "\n",
    "On or about the 16th of May 2016, all the SIPs from HDFS were scanned and opened up to extract the 153,041 known WARCs of content submitted to DLS. The `ids.py` script used to do this created an `identifiers.txt` file that contains a JSON summary of the submitted WARCs and ZIPs.\n",
    "\n",
    "From about the 9th of August to the 14th of September, these c. 150,000 mostly WARC files were downloaded and hash-checked using a simple script (`test.sh`). i.e. we downloaded about 150TB of content from DLS over about 5 weeks. This is a pretty reasonable 50 MBps sustained over a long period without notable affecting other services.\n",
    "\n",
    "Here, we compare the recovered hashes with those from HDFS. We also compare the DLS export summary with the known HDFS content.\n",
    "\n",
    "TODO\n",
    "----\n",
    "\n",
    "- Rule out temporary timeouts/failures by re-running the gaps.\n",
    "- Make a production (Luigi-based) version of this comparison that knows about ALL items ever submitted.\n",
    "    - The HDFS ID generation may mistakenly include second-submissions of difficult cases?\n",
    "\n",
    "Part 1 - Comparing the hash results\n",
    "---------------------------------\n",
    "\n",
    "We load in the original identifiers from HDFS, and then load in the DLS results, outputting a comparison file `compare.out` where the first field in each lines is either `OK` or `KO` depending on whether the hashes matched or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'mimetype': u'application/warc', u'checksum': u'55826718a7a72878637313e33104fba6fb58990f19a025e5c977f2e82576f673b3a198d44de05735290c97668b8e775e2fce631f616bfd48eb345a17bc85540e', 'crawl_id': '2013-domain-crawl/20130916143312', u'checksum_type': u'SHA-512', u'path': u'http://dls.httpfs.wa.bl.uk:14000/webhdfs/v1/heritrix/output/warcs/crawl0-20130412144423/BL-20130422000726955-03689-23518~crawler02~8443.warc.gz?user.name=hadoop&op=OPEN', u'ark': u'ark:/81055/vdc_100000038622.0x00815c', u'size': u'1006644129'}\n",
      "vdc_100000038622.0x005a4d\n",
      "...10000...\n",
      "vdc_100000038622.0x00333d\n",
      "...20000...\n",
      "vdc_100000038622.0x000c2d\n",
      "...30000...\n",
      "vdc_100023997485.0x00dca4\n",
      "...40000...\n",
      "vdc_100023997485.0x00b594\n",
      "...50000...\n",
      "vdc_100023997485.0x008e84\n",
      "...60000...\n",
      "vdc_100023997485.0x006774\n",
      "...70000...\n",
      "vdc_100023997485.0x004064\n",
      "...80000...\n",
      "vdc_100023997485.0x001954\n",
      "...90000...\n",
      "vdc_100025743210.0x00002e\n",
      "...100000...\n",
      "WARNING! no identifiers found for daily/20151022110408\n",
      "WARNING! no identifiers found for daily/20151025120418\n",
      "WARNING! no identifiers found for daily/20151102120438\n",
      "vdc_100022569061.0x000001\n",
      "...110000...\n",
      "vdc_100022807075.0x000001\n",
      "...120000...\n",
      "vdc_100022565688.0x0003f5\n",
      "...130000...\n",
      "WARNING! no identifiers found for quarterly/20150706150301\n",
      "WARNING! no identifiers found for quarterly/20151014144929\n",
      "vdc_100027180919.0x000079\n",
      "...140000...\n",
      "vdc_100022596218.0x000003\n",
      "...150000...\n",
      "\n",
      "Now running the comparison...\n",
      "...0...\n",
      "...10000...\n",
      "...20000...\n",
      "...30000...\n",
      "...40000...\n",
      "...50000...\n",
      "...60000...\n",
      "...70000...\n",
      "...80000...\n",
      "...90000...\n",
      "...100000...\n",
      "...110000...\n",
      "...120000...\n",
      "...130000...\n",
      "...140000...\n",
      "...150000...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "warcs_by_id = {}\n",
    "hdfs_ids = set()\n",
    "with open('identifiers.txt') as f:\n",
    "    counter = 0\n",
    "    for line in f:\n",
    "        crawl_id, json_str = re.split(' ', line, maxsplit=1)\n",
    "        # Unfortunately, the source list was just printed dict()s rather than JSON, so we hack:\n",
    "        json_str = json_str.strip().replace(\"'\", '\"')\n",
    "        if json_str == 'None':\n",
    "            print(\"WARNING! no identifiers found for %s\" % crawl_id)\n",
    "            continue\n",
    "        # Build up a dict:\n",
    "        warc_info = json.loads(json_str)\n",
    "        warc_info['crawl_id'] = crawl_id\n",
    "        item_id = warc_info['ark'].replace(\"ark:/81055/\", \"\")\n",
    "        hdfs_ids.add(item_id)\n",
    "        warcs_by_id[item_id] = warc_info\n",
    "        if counter == 0:\n",
    "            print(warc_info)\n",
    "        counter += 1\n",
    "        if counter%10000 == 0:\n",
    "            print(item_id)\n",
    "            print(\"...%i...\" % counter)\n",
    "\n",
    "print(\"\\nNow running the comparison...\")\n",
    "\n",
    "id_dls_sha = {}\n",
    "with open('compare.out', 'w') as fout:\n",
    "    with open('test.clean.out') as f:\n",
    "        counter = 0\n",
    "        for line in f:\n",
    "            if counter%10000 == 0:\n",
    "                print(\"...%i...\" % counter)\n",
    "            counter += 1\n",
    "            # ...\n",
    "            sha, item_id = re.split(' ', line, maxsplit=1)\n",
    "            item_id = item_id.lstrip('*').strip()\n",
    "            # Store:\n",
    "            id_dls_sha[item_id] = sha\n",
    "            # Loop\n",
    "            if item_id in warcs_by_id:\n",
    "                original_sha = warcs_by_id[item_id]['checksum']\n",
    "                original_path = warcs_by_id[item_id]['path']\n",
    "            else:\n",
    "                original_sha = None\n",
    "                original_path = None\n",
    "            if sha == original_sha:\n",
    "                decision = \"OK\"\n",
    "            else:\n",
    "                decision = \"KO\"\n",
    "            #\n",
    "            fout.write(\"%s\\t%s\\t%s\\n\" % (decision, item_id, json.dumps(warcs_by_id.get(item_id,dict()))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This comparison file can then be processed further using grep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2 - Looking at the DLS Export\n",
    "--------------------------------\n",
    "\n",
    "Here we load a copy of the export file from DLS that summarises the state according to the Boston Spa node.\n",
    "\n",
    "We parse the replication status bitmask and turn it into a replication count. We store the lookup table of known identifiers too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...0...\n",
      "...100000...\n",
      "...200000...\n",
      "...300000...\n",
      "...400000...\n",
      "...500000...\n",
      "...600000...\n",
      "(698748, 698748, 698748)\n",
      "{0: 106, 1: 28, 2: 3, 3: 1382, 4: 697229}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "replication = {}\n",
    "id_rep = {}\n",
    "identifiers = set()\n",
    "dom_ids = set()\n",
    "with open('Public Web Archive Access Export.txt') as f:\n",
    "    counter = 0\n",
    "    for line in f:\n",
    "        if counter%100000 == 0:\n",
    "            print(\"...%i...\" % counter)\n",
    "        counter += 1\n",
    "        parts = line.strip().split('\\t')\n",
    "        if len(parts) == 4:\n",
    "            item_id, bs_url, check_date, rep_status = parts\n",
    "            rep_status = int(rep_status)\n",
    "        else:\n",
    "            item_id, bs_url = parts\n",
    "            check_date = None\n",
    "            rep_status = 0\n",
    "        identifiers.add(item_id)\n",
    "        # dom_id\n",
    "        dom_id = bs_url[20:]\n",
    "        dom_ids.add(dom_id)\n",
    "        # Replication count:\n",
    "        rep_count = 0\n",
    "        for i in range(0,4):\n",
    "            bitmask = 1 << i\n",
    "            if bitmask&rep_status:\n",
    "                rep_count += 1\n",
    "        # printy\n",
    "        #if rep_count == 0 or counter%100000 == 0:\n",
    "        #    print(rep_count)\n",
    "        #    print(item_id, dom_id, check_date, rep_status)\n",
    "        # Store replication count for each ID:\n",
    "        id_rep[item_id] = rep_count\n",
    "        # Sum up\n",
    "        rep_key = rep_count\n",
    "        rep_key_count = replication.get(rep_key,0)\n",
    "        replication[rep_key] = rep_key_count + 1\n",
    "        \n",
    "print(len(identifiers), len(dom_ids), counter)\n",
    "print(replication)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data can then be plotted to make it easier to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "labels": [
          "Fully replicated",
          "Under-replicated",
          "Not replicated"
         ],
         "type": "pie",
         "values": [
          697229,
          1413,
          106
         ]
        }
       ],
       "layout": {
        "title": "Overall replication status of items in DLS"
       }
      },
      "text/html": [
       "<div id=\"ab001a94-d21b-436b-b93b-02edc4c4d69b\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"ab001a94-d21b-436b-b93b-02edc4c4d69b\", [{\"type\": \"pie\", \"labels\": [\"Fully replicated\", \"Under-replicated\", \"Not replicated\"], \"values\": [697229, 1413, 106]}], {\"title\": \"Overall replication status of items in DLS\"}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "labels": [
          0,
          1,
          2,
          3
         ],
         "sort": false,
         "type": "pie",
         "values": [
          106,
          28,
          3,
          1382
         ]
        }
       ],
       "layout": {
        "title": "Replication level of the under-replicated items in DLS"
       }
      },
      "text/html": [
       "<div id=\"d630bd7a-47f4-4dff-a8e1-a91925e027af\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"d630bd7a-47f4-4dff-a8e1-a91925e027af\", [{\"sort\": false, \"type\": \"pie\", \"labels\": [0, 1, 2, 3], \"values\": [106, 28, 3, 1382]}], {\"title\": \"Replication level of the under-replicated items in DLS\"}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "py.init_notebook_mode(connected=True)\n",
    "\n",
    "labels = ['Fully replicated', 'Under-replicated', 'Not replicated']\n",
    "values = [replication[4], 0, replication[0]]\n",
    "for rep_count in range(1,4):\n",
    "    values[1] += replication[rep_count]\n",
    "\n",
    "data = [go.Pie(\n",
    "            labels=labels,\n",
    "            values=values\n",
    "    )]\n",
    "\n",
    "py.iplot({ 'data': data, 'layout': {'title': 'Overall replication status of items in DLS'}})\n",
    "\n",
    "labels = []\n",
    "values = []\n",
    "for rep_count in sorted(replication.keys()):\n",
    "    if rep_count != 4:\n",
    "        labels.append(rep_count)\n",
    "        values.append(replication[rep_count])\n",
    "\n",
    "data = [go.Pie(\n",
    "            labels=labels,\n",
    "            values=values, \n",
    "            sort=False\n",
    "    )]\n",
    "\n",
    "py.iplot({ 'data': data, 'layout': {'title': 'Replication level of the under-replicated items in DLS'}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that although it's a relatively small percentage of the whole, there still ~1,400 items that are known to the system but not fully replicated.\n",
    "\n",
    "We can also use this data to track the items we know we submitted, i.e. comparing HDFS and DLS holdings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 14, 1: 27, 3: 175, 4: 150557, -1: 2263}\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "labels": [
          "Fully replicated",
          "Under-replicated",
          "Not replicated",
          "Missing"
         ],
         "type": "pie",
         "values": [
          150557,
          202,
          14,
          2263
         ]
        }
       ],
       "layout": {
        "title": "Overall replication status of known submitted items"
       }
      },
      "text/html": [
       "<div id=\"83dfe088-5b7b-45d0-80eb-a4896ee3eedd\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"83dfe088-5b7b-45d0-80eb-a4896ee3eedd\", [{\"type\": \"pie\", \"labels\": [\"Fully replicated\", \"Under-replicated\", \"Not replicated\", \"Missing\"], \"values\": [150557, 202, 14, 2263]}], {\"title\": \"Overall replication status of known submitted items\"}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rep_all = {}\n",
    "for item_id in warcs_by_id:\n",
    "    if item_id in identifiers:\n",
    "        rep_status = id_rep[item_id]\n",
    "    else:\n",
    "        rep_status = -1\n",
    "    count = rep_all.get(rep_status, 0)\n",
    "    rep_all[rep_status] = count + 1\n",
    "\n",
    "print(rep_all)\n",
    "\n",
    "labels = ['Fully replicated', 'Under-replicated', 'Not replicated', 'Missing']\n",
    "values = [rep_all[4], 0, rep_all[0], rep_all[-1]]\n",
    "for rep_count in range(1,4):\n",
    "    values[1] += rep_all.get(rep_count, 0)\n",
    "\n",
    "data = [go.Pie(\n",
    "            labels=labels,\n",
    "            values=values\n",
    "    )]\n",
    "\n",
    "py.iplot({ 'data': data, 'layout': {'title': 'Overall replication status of submitted items'}})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now we see that there are significantly more items that are known, and that we believe were submitted to DLS, but are currently completely unknown to the system.\n",
    "\n",
    "That said, we can also run the comparison the other way, and find there are many more DLS items not covered by the current analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(547975, 698748, 153036)\n"
     ]
    }
   ],
   "source": [
    "dls_only_ids = []\n",
    "for item_id in identifiers:\n",
    "    if item_id not in hdfs_ids:\n",
    "        dls_only_ids.append(item_id)\n",
    "        \n",
    "print(len(dls_only_ids), len(identifiers), len(hdfs_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is _mainly_ because this analysis does not cover _any_ WCT content, but we need a full picture of what we've submitted in order to carry this analysis out completely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
